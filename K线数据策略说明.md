# K线数据获取与使用策略说明

## 数据获取原则

### 1. 初始获取（全量）
**获取天数**: 150个交易日（约180个自然日）

**原因**:
- 计算MA120（120日均线）需要至少120条K线数据
- 计算其他长期技术指标需要足够的历史数据
- 留有余量，防止停牌等情况导致数据不足

**实现位置**:
- `comprehensive_data_collector.py`: `self.kline_days = 150`
- `get_choice_data.py`: `start_date = (datetime.now() - timedelta(days=180))`

### 2. 增量更新
**更新策略**: 只获取最后更新日期到当前日期之间的新数据

**实现方式**:
1. 读取数据文件中最后一条K线的日期
2. 从该日期+1天开始获取到今天
3. 合并到现有数据中

**好处**:
- 节省API调用次数和配额
- 提高更新速度
- 减少网络传输量

## 数据使用原则

### 按需使用不同长度的数据

#### 短期技术指标（1-30天）
- **RSI** (相对强弱指标): 使用最近14-30天
- **MACD**: 使用最近30-60天
- **MA5/MA10/MA20**: 使用最近30-60天
- **成交量分析**: 使用最近30天

**使用场景**: 短期预测（1-7天）、筹码健康度分析

#### 中期技术指标（30-90天）
- **MA60**: 使用最近90天
- **中期趋势**: 使用60-90天

**使用场景**: 中期预测（1-3个月）

#### 长期技术指标（120+天）
- **MA120**: 使用全部150天
- **长期趋势**: 使用120-150天

**使用场景**: 长期预测（3-12个月）

### 具体应用示例

#### 筹码健康度分析
```python
# 只使用最近30-60天的数据
kline_data = stock_data['kline_data']['daily'][-60:]  # 取最后60条
```

#### 技术指标计算
```python
# 短期指标：使用30-60天
short_term_data = kline_data[-60:]

# 中期指标：使用90天  
medium_term_data = kline_data[-90:]

# 长期指标：使用全部150天
long_term_data = kline_data  # 全部数据
```

## 数据质量检查

### 最小数据要求
- **短期分析**: 至少30条K线数据
- **中期分析**: 至少60条K线数据
- **长期分析**: 至少120条K线数据

### 数据不足的处理
当K线数据不足时：
1. 在日志中明确警告：`⚠️ [股票代码] 警告：K线数据不足(<120条)`
2. 标注可能的原因：数据源问题、新股上市、长期停牌等
3. 根据可用数据长度决定可以计算哪些指标
4. 对于无法计算的指标返回默认值或跳过

## 性能优化

### 批量获取
- Choice CSD接口: 100只股票/批次（用逗号分隔股票代码）
- Choice CSS接口: 单个请求但支持批量查询
- 其他API: 根据各自限制调整

### 缓存策略
- 使用文件缓存存储已获取的历史数据
- 每日只更新当天的新数据
- 定期清理过期数据（保留最近180天）

## 错误处理

### 常见问题
1. **K线数据为空**: 检查股票代码、日期范围、API权限
2. **数据长度不足**: 新股、停牌、退市等情况
3. **API配额耗尽**: 切换到备用数据源或使用缓存数据

### 降级策略
1. Choice CSD (首选) → Choice CSS (备选1) → 其他API (备选2) → 缓存数据 (兜底)
2. 实时数据 → 缓存数据 → 模拟数据（仅用于测试）

## 代码实现检查清单

- [x] `comprehensive_data_collector.py`: kline_days = 150
- [x] `get_choice_data.py`: 主函数使用180天（150交易日）
- [x] `get_choice_data.py`: 测试函数使用180天
- [ ] 增量更新逻辑实现
- [x] 数据不足警告和原因说明
- [ ] 按需使用不同长度数据的示例代码

## 下一步改进

1. **实现增量更新**: 检查最后一条K线日期，只获取新数据
2. **数据验证**: 检查K线数据的连续性和有效性
3. **智能降级**: 根据数据可用性自动选择合适的分析方法
4. **数据清理**: 定期删除过期的历史数据
