# ==================== æ€§èƒ½ä¼˜åŒ–å®ç°æ¨¡å— ====================
"""
æ€§èƒ½ä¼˜åŒ–å®ç°æ¨¡å— - æä¾›ç¼“å­˜ã€å¼‚æ­¥å¤„ç†å’Œä¼˜åŒ–åˆ†æåŠŸèƒ½
ä¸ºAè‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿæä¾›æ€§èƒ½å¢å¼º
"""

import asyncio
import hashlib
import json
import threading
import time
from collections import OrderedDict, defaultdict
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Tuple


class HighPerformanceCache:
    """é«˜æ€§èƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = OrderedDict()
        self.timestamps = {}
        self.hit_count = 0
        self.miss_count = 0
        self._lock = threading.RLock()
        
    def _is_expired(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜é¡¹æ˜¯å¦è¿‡æœŸ"""
        if key not in self.timestamps:
            return True
        return (datetime.now() - self.timestamps[key]).seconds > self.ttl_seconds
        
    def _cleanup_expired(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜é¡¹"""
        current_time = datetime.now()
        expired_keys = [
            key for key, timestamp in self.timestamps.items()
            if (current_time - timestamp).seconds > self.ttl_seconds
        ]
        for key in expired_keys:
            self.cache.pop(key, None)
            self.timestamps.pop(key, None)
            
    def _make_key(self, *args, **kwargs) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()
        
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        with self._lock:
            if key in self.cache and not self._is_expired(key):
                # ç§»åŠ¨åˆ°æœ«å°¾ï¼ˆLRUï¼‰
                value = self.cache.pop(key)
                self.cache[key] = value
                self.hit_count += 1
                return value
            else:
                self.miss_count += 1
                return None
                
    def put(self, key: str, value: Any):
        """å­˜å‚¨ç¼“å­˜å€¼"""
        with self._lock:
            # æ¸…ç†è¿‡æœŸé¡¹
            self._cleanup_expired()
            
            # å¦‚æœç¼“å­˜æ»¡äº†ï¼Œåˆ é™¤æœ€è€çš„é¡¹
            if len(self.cache) >= self.max_size:
                oldest_key = next(iter(self.cache))
                self.cache.pop(oldest_key)
                self.timestamps.pop(oldest_key, None)
                
            self.cache[key] = value
            self.timestamps[key] = datetime.now()
            
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self.hit_count + self.miss_count
        hit_rate = (self.hit_count / total_requests) if total_requests > 0 else 0
        
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'hit_count': self.hit_count,
            'miss_count': self.miss_count,
            'hit_rate': round(hit_rate, 3),
            'ttl_seconds': self.ttl_seconds
        }
        
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.timestamps.clear()
            self.hit_count = 0
            self.miss_count = 0


class AsyncDataProcessor:
    """å¼‚æ­¥æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self, cache: HighPerformanceCache = None):
        self.cache = cache or HighPerformanceCache()
        self.executor = None
        self.max_workers = 5
        
    def start_executor(self):
        """å¯åŠ¨çº¿ç¨‹æ± æ‰§è¡Œå™¨"""
        if self.executor is None:
            from concurrent.futures import ThreadPoolExecutor
            self.executor = ThreadPoolExecutor(max_workers=self.max_workers)
            
    def stop_executor(self):
        """åœæ­¢çº¿ç¨‹æ± æ‰§è¡Œå™¨"""
        if self.executor:
            self.executor.shutdown(wait=True)
            self.executor = None
            
    def submit_task(self, func, *args, **kwargs):
        """æäº¤å¼‚æ­¥ä»»åŠ¡"""
        self.start_executor()
        return self.executor.submit(func, *args, **kwargs)
        
    def batch_process(self, func, items: List[Any], batch_size: int = 10) -> List[Any]:
        """æ‰¹é‡å¤„ç†æ•°æ®"""
        results = []
        
        # æ£€æŸ¥ç¼“å­˜
        for item in items:
            cache_key = self.cache._make_key(func.__name__, item)
            cached_result = self.cache.get(cache_key)
            if cached_result is not None:
                results.append(cached_result)
            else:
                # éœ€è¦å¤„ç†çš„é¡¹
                try:
                    result = func(item)
                    self.cache.put(cache_key, result)
                    results.append(result)
                except Exception as e:
                    print(f"å¤„ç†é¡¹ç›® {item} æ—¶å‡ºé”™: {e}")
                    results.append(None)
                    
        return results
        
    def parallel_process(self, func, items: List[Any], max_workers: int = None) -> List[Any]:
        """å¹¶è¡Œå¤„ç†æ•°æ®"""
        if max_workers:
            self.max_workers = max_workers
            
        self.start_executor()
        futures = []
        
        for item in items:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = self.cache._make_key(func.__name__, item)
            cached_result = self.cache.get(cache_key)
            if cached_result is not None:
                futures.append(None)  # å ä½ç¬¦
            else:
                future = self.executor.submit(func, item)
                futures.append(future)
                
        # æ”¶é›†ç»“æœ
        results = []
        for i, future in enumerate(futures):
            if future is None:
                # ä½¿ç”¨ç¼“å­˜ç»“æœ
                cache_key = self.cache._make_key(func.__name__, items[i])
                results.append(self.cache.get(cache_key))
            else:
                try:
                    result = future.result(timeout=30)
                    cache_key = self.cache._make_key(func.__name__, items[i])
                    self.cache.put(cache_key, result)
                    results.append(result)
                except Exception as e:
                    print(f"å¹¶è¡Œå¤„ç†é¡¹ç›® {items[i]} æ—¶å‡ºé”™: {e}")
                    results.append(None)
                    
        return results


class OptimizedStockAnalyzer:
    """ä¼˜åŒ–çš„è‚¡ç¥¨åˆ†æå™¨"""
    
    def __init__(self, cache: HighPerformanceCache = None):
        self.cache = cache or HighPerformanceCache()
        self.processor = AsyncDataProcessor(self.cache)
        
    def analyze_stock_batch(self, stock_codes: List[str], analysis_func) -> Dict[str, Any]:
        """æ‰¹é‡åˆ†æè‚¡ç¥¨"""
        start_time = time.time()
        
        # å¹¶è¡Œå¤„ç†è‚¡ç¥¨åˆ†æ
        results = self.processor.parallel_process(analysis_func, stock_codes)
        
        # ç»„ç»‡ç»“æœ
        analysis_results = {}
        for code, result in zip(stock_codes, results):
            if result is not None:
                analysis_results[code] = result
                
        end_time = time.time()
        processing_time = round(end_time - start_time, 2)
        
        return {
            'results': analysis_results,
            'stats': {
                'total_stocks': len(stock_codes),
                'successful_analyses': len(analysis_results),
                'processing_time_seconds': processing_time,
                'cache_stats': self.cache.get_stats()
            }
        }
        
    def get_cached_analysis(self, stock_code: str, analysis_type: str = "default") -> Optional[Any]:
        """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
        cache_key = self.cache._make_key(stock_code, analysis_type)
        return self.cache.get(cache_key)
        
    def cache_analysis_result(self, stock_code: str, result: Any, analysis_type: str = "default"):
        """ç¼“å­˜åˆ†æç»“æœ"""
        cache_key = self.cache._make_key(stock_code, analysis_type)
        self.cache.put(cache_key, result)
        
    def optimize_data_loading(self, data_loader_func, data_keys: List[str]) -> Dict[str, Any]:
        """ä¼˜åŒ–æ•°æ®åŠ è½½"""
        # æ£€æŸ¥å“ªäº›æ•°æ®å·²ç»ç¼“å­˜
        cached_data = {}
        missing_keys = []
        
        for key in data_keys:
            cache_key = self.cache._make_key("data_load", key)
            cached_result = self.cache.get(cache_key)
            if cached_result is not None:
                cached_data[key] = cached_result
            else:
                missing_keys.append(key)
                
        # åªåŠ è½½ç¼ºå¤±çš„æ•°æ®
        if missing_keys:
            new_data = self.processor.parallel_process(data_loader_func, missing_keys)
            
            # ç¼“å­˜æ–°æ•°æ®
            for key, data in zip(missing_keys, new_data):
                if data is not None:
                    cache_key = self.cache._make_key("data_load", key)
                    self.cache.put(cache_key, data)
                    cached_data[key] = data
                    
        return cached_data
        
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.processor.stop_executor()
        self.cache.clear()


# ä¾¿æ·å‡½æ•°
def create_optimized_system() -> Tuple[HighPerformanceCache, AsyncDataProcessor, OptimizedStockAnalyzer]:
    """åˆ›å»ºä¼˜åŒ–ç³»ç»Ÿçš„ä¾¿æ·å‡½æ•°"""
    cache = HighPerformanceCache(max_size=2000, ttl_seconds=7200)  # 2å°æ—¶TTL
    processor = AsyncDataProcessor(cache)
    analyzer = OptimizedStockAnalyzer(cache)
    
    return cache, processor, analyzer


def benchmark_performance(func, *args, **kwargs) -> Dict[str, Any]:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    start_time = time.time()
    start_memory = 0  # ç®€åŒ–ç‰ˆæœ¬ï¼Œä¸å®é™…æµ‹é‡å†…å­˜
    
    try:
        result = func(*args, **kwargs)
        success = True
        error = None
    except Exception as e:
        result = None
        success = False
        error = str(e)
        
    end_time = time.time()
    execution_time = round(end_time - start_time, 3)
    
    return {
        'success': success,
        'execution_time_seconds': execution_time,
        'result': result,
        'error': error,
        'function_name': func.__name__
    }


# æ¨¡å—ç‰ˆæœ¬ä¿¡æ¯
__version__ = "1.0.0"
__author__ = "Aè‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ"
__description__ = "æ€§èƒ½ä¼˜åŒ–å®ç°æ¨¡å—"

# å¯¼å‡ºçš„ä¸»è¦ç±»å’Œå‡½æ•°
__all__ = [
    'HighPerformanceCache',
    'AsyncDataProcessor', 
    'OptimizedStockAnalyzer',
    'create_optimized_system',
    'benchmark_performance'
]


if __name__ == "__main__":
    # æµ‹è¯•æ¨¡å—åŠŸèƒ½
    print("ğŸš€ æ€§èƒ½ä¼˜åŒ–æ¨¡å—æµ‹è¯•")
    
    # æµ‹è¯•ç¼“å­˜
    cache = HighPerformanceCache(max_size=5)
    cache.put("test1", {"data": "value1"})
    cache.put("test2", {"data": "value2"})
    
    print("ç¼“å­˜æµ‹è¯•:")
    print(f"è·å–test1: {cache.get('test1')}")
    print(f"ç¼“å­˜ç»Ÿè®¡: {cache.get_stats()}")
    
    # æµ‹è¯•å¼‚æ­¥å¤„ç†å™¨
    processor = AsyncDataProcessor(cache)
    
    def test_func(x):
        time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        return x * 2
        
    print("\nå¹¶è¡Œå¤„ç†æµ‹è¯•:")
    start_time = time.time()
    results = processor.parallel_process(test_func, [1, 2, 3, 4, 5])
    end_time = time.time()
    print(f"ç»“æœ: {results}")
    print(f"å¤„ç†æ—¶é—´: {round(end_time - start_time, 2)} ç§’")
    
    # æµ‹è¯•ä¼˜åŒ–åˆ†æå™¨
    analyzer = OptimizedStockAnalyzer(cache)
    
    def mock_stock_analysis(stock_code):
        time.sleep(0.05)  # æ¨¡æ‹Ÿåˆ†ææ—¶é—´
        return {
            'code': stock_code,
            'score': 8.5,
            'recommendation': 'buy'
        }
    
    print("\nè‚¡ç¥¨æ‰¹é‡åˆ†ææµ‹è¯•:")
    stocks = ['000001', '000002', '600000']
    batch_results = analyzer.analyze_stock_batch(stocks, mock_stock_analysis)
    print(f"åˆ†æç»“æœ: {batch_results['stats']}")
    
    # æ¸…ç†
    processor.stop_executor()
    
    print("\nâœ… æ€§èƒ½ä¼˜åŒ–æ¨¡å—æµ‹è¯•å®Œæˆ")